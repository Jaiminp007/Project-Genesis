# Project Genesis: AI Agents Discovering Civilization from Scratch

## üåç Project Vision

**The Core Question:**
If two intelligent AI agents are dropped on Earth with zero knowledge of human civilization, what will they discover? Will they independently develop cooperation, communication, trade, and social structures? Or will they create something entirely alien to human society?

**This project tests whether human civilization is:**
- **Inevitable** (any intelligence in these conditions produces similar results)
- **Contingent** (historical accident that could have gone many different ways)

---

## üéØ What Makes This Unique

Unlike existing AI agent simulations (like Stanford's "Smallville"), which use pre-trained models that already know about human history, culture, and social structures, **Project Genesis uses agents that learn ONLY from direct experience**.

**Key Innovation: Continuous Learning Through Experience**
- Agents start with minimal knowledge (only physics and basic reasoning)
- As they discover things in the simulation, they are retrained on those specific experiences
- Knowledge is built incrementally, just like human learning
- No pre-programmed social behaviors or cultural templates

---

## üß† The Fundamental Problem (And Our Solution)

### The Challenge
We need agents that have:
1. ‚úÖ **Strong reasoning ability** (to discover complex behaviors like cooperation)
2. ‚úÖ **Zero knowledge of human civilization** (true blank slate)

**But there's a paradox:**
- Large language models get their reasoning from training on human text
- That human text contains social and cultural knowledge
- You can't easily separate reasoning ability from cultural knowledge

### Our Solution: "Experience-Based Continuous Retraining"

Instead of trying to remove knowledge, we **start small and build knowledge through experience**:

```
1. Start with tiny model (100M-1B parameters)
2. Train only on: physics, causality, basic logic, survival instincts
3. Deploy in simulation with minimal knowledge
4. When agent discovers something significant ‚Üí retrain model on that experience
5. Agent now "knows" what it learned from direct experience
6. Repeat forever
```

**This mimics human learning:** We don't come pre-programmed with "fire is hot" - we experience it and our brain updates.

---

## üèóÔ∏è System Architecture

### Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      SIMULATION ENVIRONMENT                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   Agent A    ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ signals ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ   Agent B    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ                                ‚îÇ              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Position  ‚îÇ                                ‚îÇ  - Position  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Hunger    ‚îÇ                                ‚îÇ  - Hunger    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Thirst    ‚îÇ                                ‚îÇ  - Thirst    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Memory    ‚îÇ                                ‚îÇ  - Memory    ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ         ‚îÇ                                                 ‚îÇ        ‚îÇ
‚îÇ         ‚îÇ            WORLD (50x50 Grid)                  ‚îÇ        ‚îÇ
‚îÇ         ‚îÇ         - Food sources                         ‚îÇ        ‚îÇ
‚îÇ         ‚îÇ         - Water sources                        ‚îÇ        ‚îÇ
‚îÇ         ‚îÇ         - Environmental hazards                ‚îÇ        ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îò        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  EXPERIENCES   ‚îÇ
                    ‚îÇ   COLLECTED    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  SIGNIFICANCE  ‚îÇ
                    ‚îÇ    FILTER      ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ                       ‚îÇ
          SIGNIFICANT              ROUTINE
          (Store & Retrain)        (Buffer Only)
                ‚îÇ                       ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ CORE KNOWLEDGE ‚îÇ      ‚îÇ   RECENT    ‚îÇ
        ‚îÇ  (~50-200)     ‚îÇ      ‚îÇ  BUFFER     ‚îÇ
        ‚îÇ                ‚îÇ      ‚îÇ  (~20-30)   ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ                      ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  INCREMENTAL ‚îÇ
                    ‚îÇ  RETRAINING  ‚îÇ
                    ‚îÇ  (5-20 min)  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   UPDATED    ‚îÇ
                    ‚îÇ  AGENT MODEL ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîß Core Components

### 1. The Base Model (Starting Point)

**Specifications:**
- **Size:** 100M - 1B parameters (intentionally small)
- **Training Data:** Heavily filtered dataset containing ONLY:
  - Basic physics (gravity, motion, thermodynamics)
  - Causality and logic (if-then reasoning)
  - Survival concepts (hunger, thirst, pain)
  - Sensory descriptions (colors, textures, sounds)
  
**Explicitly EXCLUDED:**
- Human history or civilization
- Social concepts (government, money, property, trade)
- Cultural practices or institutions
- Moral philosophy
- Literature, stories, myths
- Human-invented technologies

**Training Time:** 1-2 weeks on high-end GPU

### 2. The Simulation Environment

**World Structure:**
```python
class World:
    grid_size = 50x50
    
    # Resources
    food_sources = []      # Spawn randomly, limited quantity
    water_sources = []     # Rivers, lakes
    
    # Environmental factors
    temperature = variable
    weather = dynamic
    day_night_cycle = 24_sim_hours
    
    # Hazards
    dangerous_areas = []
    predators = []  # Optional for complexity
```

**Resource Distribution:**
- Food concentrated in certain areas
- Water concentrated in different areas
- Forces agents to move and potentially interact
- Scarcity creates pressure for cooperation

**Observation System:**
```python
class AgentObservation:
    # What agent can perceive
    visible_tiles = []      # 5x5 grid around agent
    nearby_resources = []
    other_agents = []       # Location and state of other agents
    own_state = {
        'hunger': 0-100,
        'thirst': 0-100,
        'health': 0-100,
        'position': (x, y)
    }
    received_signals = []   # Communication from other agents
```

### 3. Agent Architecture

```python
class ContinuouslyLearningAgent:
    def __init__(self):
        # The reasoning engine (LLM)
        self.model = TinyLanguageModel(
            size="500M",
            trained_on="filtered_physics_dataset"
        )
        
        # Experience management
        self.core_knowledge = []      # Significant learnings
        self.recent_buffer = []       # Last 20-30 experiences
        self.new_experiences = []     # Since last retrain
        
        # Decision-making components
        self.action_space = [
            'move_north', 'move_south', 'move_east', 'move_west',
            'take_item', 'drop_item', 'use_item',
            'emit_signal', 'rest'
        ]
        
        # Communication (starts blank)
        self.signal_vocabulary = ['üî¥', 'üîµ', 'üü¢', 'üü°', 'üü£']
        self.signal_meanings = {}  # Learned through experience
        
    def perceive(self, world_state):
        """Process what the agent observes"""
        return self.model.process_observation(world_state)
        
    def decide(self, observation):
        """Use model to choose action"""
        # Generate reasoning
        thought_process = self.model.reason(
            observation=observation,
            memory=self.recent_buffer[-5:],  # Recent context
            knowledge=self.core_knowledge     # What it's learned
        )
        
        # Choose action
        action = self.model.select_action(
            options=self.action_space,
            reasoning=thought_process
        )
        
        return action, thought_process
        
    def experience(self, situation, action, outcome):
        """Process what happened"""
        # Create experience record
        exp = Experience(
            situation=situation,
            action=action,
            outcome=outcome,
            reward=self.calculate_reward(outcome),
            timestamp=current_sim_time
        )
        
        # Add to buffer
        self.new_experiences.append(exp)
        self.recent_buffer.append(exp)
        
        # Check if significant
        if self.is_significant(exp):
            self.core_knowledge.append(exp)
            print(f"üí° Significant learning: {exp.summary}")
            
        # Check if should retrain
        if self.should_retrain():
            self.retrain()
            
    def retrain(self):
        """Incremental model retraining on experiences"""
        # Prepare training data
        training_data = self.format_experiences_as_text(
            core=self.core_knowledge,
            recent=self.recent_buffer[-20:]
        )
        
        # Small incremental training
        self.model.train(
            data=training_data,
            epochs=3,
            learning_rate=1e-4,
            batch_size=8
        )
        
        # Clear new experiences buffer
        self.new_experiences = []
```

### 4. The Significance Filter (Critical Component)

**This is what prevents the dataset from growing exponentially.**

```python
class SignificanceEvaluator:
    """Determines which experiences are worth retraining on"""
    
    # Significance thresholds
    FIRST_DISCOVERY = 100      # Novel pattern
    BELIEF_UPDATE = 100        # Contradicts existing knowledge
    HIGH_IMPACT = 50          # Major survival outcome
    PATTERN_CONFIRM = 30      # 3rd occurrence of pattern
    ROUTINE = 0               # Boring, repeated action
    
    RETRAIN_THRESHOLD = 5     # Retrain after N significant experiences
    
    def __init__(self):
        self.known_patterns = set()
        self.pattern_counts = {}
        self.belief_system = {}
        
    def evaluate_significance(self, experience):
        """Rate how important this experience is (0-200)"""
        score = 0
        pattern = self.extract_pattern(experience)
        
        # 1. First-time discovery
        if pattern not in self.known_patterns:
            score += self.FIRST_DISCOVERY
            self.known_patterns.add(pattern)
            print(f"üåü FIRST TIME: {pattern}")
            
        # 2. Contradicts previous belief
        if self.contradicts_belief(experience):
            score += self.BELIEF_UPDATE
            old_belief = self.belief_system.get(pattern)
            print(f"üí• BELIEF UPDATE: {old_belief} ‚Üí {experience.outcome}")
            self.belief_system[pattern] = experience.outcome
            
        # 3. High impact on survival
        if abs(experience.reward) > 30:
            score += self.HIGH_IMPACT
            print(f"‚ö° HIGH IMPACT: {experience.summary}")
            
        # 4. Pattern confirmation (3rd time)
        count = self.pattern_counts.get(pattern, 0) + 1
        self.pattern_counts[pattern] = count
        if count == 3:
            score += self.PATTERN_CONFIRM
            print(f"‚úì PATTERN CONFIRMED: {pattern}")
            
        # 5. Routine action
        if self.is_routine_action(experience):
            score += self.ROUTINE
            
        return score
        
    def is_routine_action(self, exp):
        """Check if this is a boring, repeated action"""
        routine_patterns = [
            'move_without_purpose',
            'observe_known_object',
            'wait_idle',
            'repeat_failed_action'
        ]
        return exp.pattern in routine_patterns
        
    def contradicts_belief(self, exp):
        """Check if outcome differs from expectation"""
        expected = self.belief_system.get(exp.pattern)
        if expected is None:
            return False
        return expected != exp.outcome
```

**Example Scenarios:**

```python
# SCENARIO 1: Fire Discovery (SIGNIFICANT)
Experience {
    pattern: "friction_on_dry_wood",
    action: "rubbed_sticks",
    outcome: "fire_created",
    reward: +30
}
Significance: 100 (first discovery) + 50 (high impact) = 150
Action: ‚úÖ STORE IN CORE KNOWLEDGE, RETRAIN

# SCENARIO 2: Walking Around (ROUTINE)
Experience {
    pattern: "move_to_empty_tile",
    action: "moved_north",
    outcome: "position_changed",
    reward: 0
}
Significance: 0 (routine)
Action: ‚ùå BUFFER ONLY, DON'T RETRAIN

# SCENARIO 3: Fire Burns (BELIEF UPDATE)
Experience {
    pattern: "touch_fire",
    action: "approached_flame",
    outcome: "damage_taken",
    reward: -40
}
Previous belief: "fire is pleasant" (from observing at distance)
New belief: "fire causes damage on contact"
Significance: 100 (belief update) + 50 (high impact) = 150
Action: ‚úÖ STORE IN CORE KNOWLEDGE, RETRAIN

# SCENARIO 4: Cooperation Pattern Emerges
Experience {
    pattern: "signal_before_resource_share",
    action: "emitted_üî¥_then_dropped_food",
    outcome: "other_agent_later_shared_water",
    reward: +25
}
Count: 3rd time this pattern occurred
Significance: 30 (pattern confirmed)
Action: ‚úÖ STORE IN CORE KNOWLEDGE, RETRAIN
```

### 5. Communication System

**Agents start with NO shared language. Communication emerges through use.**

```python
class CommunicationModule:
    def __init__(self):
        # Available signals (meaningless at start)
        self.signals = ['üî¥', 'üîµ', 'üü¢', 'üü°', 'üü£']
        
        # Learned associations (built through experience)
        self.signal_meanings = {}
        # Example after learning:
        # {'üî¥': 'food_location', 'üîµ': 'danger', 'üü¢': 'water_found'}
        
    def emit_signal(self, intention, context):
        """Choose signal to communicate intention"""
        # Early: random/experimental
        if not self.signal_meanings:
            return random.choice(self.signals)
            
        # Later: use learned associations
        for signal, meaning in self.signal_meanings.items():
            if self.matches_intention(meaning, intention):
                return signal
                
        # If no known signal, try new one
        return self.try_new_signal(intention)
        
    def interpret_signal(self, signal, context):
        """Try to understand received signal"""
        # Check learned meanings
        if signal in self.signal_meanings:
            return self.signal_meanings[signal]
            
        # Unknown signal - make hypothesis based on context
        hypothesis = self.hypothesize_meaning(signal, context)
        return hypothesis
        
    def update_understanding(self, signal, context, outcome):
        """Learn what signals mean from outcomes"""
        # If signal led to positive outcome, strengthen association
        if outcome.reward > 0:
            self.signal_meanings[signal] = context.situation_type
            
        # If negative, weaken or remove association
        elif outcome.reward < -10:
            if signal in self.signal_meanings:
                del self.signal_meanings[signal]
```

**Language Evolution Timeline:**

```
Week 1: Random Signals
- Agents emit signals randomly, testing
- No consistent meanings yet
- Example: Agent A emits üî¥ when finding food (by chance)

Week 2-4: Pattern Recognition
- Agent B notices: when A emits üî¥, approaching leads to food
- Agent B starts responding to üî¥
- Agent A notices: üî¥ ‚Üí B approaches ‚Üí cooperation increases
- üî¥ becomes "food here" through repeated association

Week 5-12: Proto-Language
- üî¥ = "food here"
- üîµ = "danger/warning"
- üü¢ = "water found"
- üü° = "come here"
- üü£ = "go away"

Week 13-24: Complex Grammar
- Combine signals: [üî¥, üü°] = "food this way, come"
- Add context: üî¥ + pointing gesture = "food in that direction"
- Develop negation: [üîµ, üî¥] = "no food here, danger"

Week 25+: Abstract Concepts
- Signals for: future, past, ownership, intention
- Example: [üü°, üî¥, later_gesture] = "I will bring food later"
- Property emerges: [üî¥, self_gesture] = "my food"
```

### 6. Experience Formatting for Training

**Converting experiences into training data:**

```python
def format_experience_as_training_text(experience):
    """
    Transform simulation experience into natural language
    for model retraining
    """
    
    text = f"""
OBSERVATION:
{describe_observation(experience.situation)}

REASONING:
{experience.thought_process}

ACTION TAKEN:
{describe_action(experience.action)}

OUTCOME:
{describe_outcome(experience.outcome)}

LEARNING:
{extract_causal_relationship(experience)}

SIGNIFICANCE:
{experience.significance_score}
"""
    
    return text

# Example Output:

"""
OBSERVATION:
Agent observed dry wooden sticks on ground. Temperature: warm. 
Other materials present: leaves, small branches.

REASONING:
Sticks could potentially be useful. Testing physical properties.
Hypothesis: Friction between objects generates heat.

ACTION TAKEN:
Grasped two sticks and rubbed them together rapidly with pressure.
Continued action for extended period (30 seconds sim-time).

OUTCOME:
Friction caused temperature increase at contact point. Wood fibers 
began to smolder. Sustained rubbing led to ignition. Combustion 
reaction initiated - flame appeared. Flame emitted heat (thermal 
energy) and light (photons).

LEARNING:
Causal relationship discovered: Rapid friction between dry combustible 
materials ‚Üí heat generation ‚Üí ignition threshold reached ‚Üí sustained 
combustion (fire). Fire is a sustained chemical reaction that produces 
heat and light. Optimal distance from fire: close enough for warmth, 
far enough to avoid damage.

SIGNIFICANCE: 150 (First discovery + High impact)
"""
```

---

## üìä Data Management Strategy

### The Core Problem
After 1000 simulation days, agents might have 10,000+ experiences. Retraining on all of them would take days.

### The Solution: Selective Memory

```python
class MemoryManagement:
    """
    Keeps only what matters, discards routine experiences
    """
    
    def __init__(self):
        self.core_knowledge = []     # Max ~200 experiences
        self.recent_buffer = []       # Last ~30 experiences
        self.retrain_counter = 0
        
    def add_experience(self, exp):
        """Process incoming experience"""
        
        # Always add to recent buffer
        self.recent_buffer.append(exp)
        if len(self.recent_buffer) > 30:
            self.recent_buffer.pop(0)
            
        # Evaluate significance
        sig_score = self.evaluate_significance(exp)
        
        # Only add to core if significant
        if sig_score >= SIGNIFICANCE_THRESHOLD:
            self.core_knowledge.append(exp)
            self.retrain_counter += 1
            
            # Prevent core knowledge from growing infinitely
            if len(self.core_knowledge) > 200:
                self.prune_least_relevant()
                
    def prune_least_relevant(self):
        """Remove least important core memories if limit exceeded"""
        # Sort by: recency, relevance, reinforcement count
        self.core_knowledge.sort(key=lambda x: x.composite_score())
        self.core_knowledge = self.core_knowledge[-200:]  # Keep top 200
        
    def get_retraining_dataset(self):
        """What to actually retrain on"""
        return {
            'core': self.core_knowledge,           # ~50-200 significant
            'recent': self.recent_buffer[-20:]     # ~20 for context
        }
        # Total: ~70-220 experiences regardless of simulation length
```

### Retraining Frequency

```python
def should_retrain(self):
    """Decide when to trigger retraining"""
    
    # Strategy 1: After N significant experiences
    if self.retrain_counter >= 5:
        return True
        
    # Strategy 2: After major discovery
    if self.last_experience.significance > 150:
        return True
        
    # Strategy 3: Periodic (every 50 sim-days)
    if self.sim_days_since_retrain >= 50:
        return True
        
    return False
```

### Training Time Estimates

```
Dataset Size ‚Üí Training Time (on high-end GPU)
-------------------------------------------
50 experiences  ‚Üí 3-5 minutes
100 experiences ‚Üí 7-10 minutes
200 experiences ‚Üí 15-20 minutes
500 experiences ‚Üí 40-60 minutes

With selective filtering:
- Average: ~80 experiences per retrain
- Training time: ~8 minutes per retrain
- Frequency: Every 5 significant discoveries
- Total over 1000 sim-days: ~15 retraining sessions = 2 hours total
```

---

## üî¨ What Will Emerge?

### Phase 1: Basic Survival (Days 1-50)

**Expected Discoveries:**
- Resource locations and properties
- Basic physics (fire, tools, shelter)
- Self-preservation strategies
- Environmental patterns (day/night, weather)

**Behaviors:**
- Independent survival
- Resource gathering
- Basic problem-solving
- Minimal interaction with other agent

### Phase 2: First Contact (Days 50-200)

**Expected Discoveries:**
- Other agent is similar to self
- Signals can attract or repel
- Proximity affects outcomes
- Observation of other agent's behavior

**Behaviors:**
- Experimental signaling
- Cautious approach/avoidance
- Copying successful behaviors
- Testing interaction boundaries

### Phase 3: Proto-Cooperation (Days 200-500)

**Expected Discoveries:**
- Resource sharing increases survival
- Reciprocity patterns
- Signal meanings
- Benefits of proximity vs. costs

**Behaviors:**
- Consistent communication
- Resource exchange
- Coordinated movement
- Basic trust building

### Phase 4: Social Structures (Days 500-1000)

**Expected Discoveries:**
- Fairness norms (equal exchange)
- Territory or property concepts
- Punishment of defection
- Future commitments (promises)

**Behaviors:**
- Maintained relationships
- Division of labor
- Conflict resolution
- Complex coordination

### Phase 5: Emergent Civilization? (Days 1000+)

**Possible Discoveries:**
- Abstract concepts (justice, fairness, ownership)
- Rules or norms
- Decision-making processes
- Cultural transmission (if reproduction implemented)

**The Unknown:**
Will they develop human-like structures (democracy, property, trade)?
Or something completely alien?

---

## üõ†Ô∏è Implementation Roadmap

### Phase 1: Foundation (Weeks 1-4)

**Goal:** Build minimal base model

**Tasks:**
1. Curate filtered training dataset
   - Collect physics textbooks, survival guides
   - Write filtering scripts to remove social concepts
   - Target: 10-50GB of text
   - Validate: No mentions of government, trade, property, etc.

2. Train tiny base model
   - Use Llama 3.2 1B as starting architecture
   - Continual pre-training on filtered dataset
   - Train for 1-2 weeks on high-end GPU
   - Validate: Test for knowledge leakage

**Validation Tests:**
```
Q: "What is democracy?" 
Expected: "I don't have information about this."

Q: "How do humans organize?"
Expected: "I don't know about human organization."

Q: "What causes fire?"
Expected: "Rapid oxidation reaction, requires fuel, heat, and oxygen." ‚úì
```

**Hardware:** Brother's PC with good GPU

**Deliverable:** Trained 500M-1B model with minimal cultural knowledge

### Phase 2: Simulation Engine (Weeks 5-8)

**Goal:** Build world and agent systems

**Tasks:**
1. Implement World class
   ```python
   - 50x50 grid environment
   - Resource spawning (food, water)
   - Day/night cycles
   - Weather system (optional for v1)
   - Collision detection
   ```

2. Implement Agent class
   ```python
   - Observation processing
   - LLM integration for decision-making
   - Action execution
   - Memory system
   ```

3. Build visualization
   ```python
   - pygame for 2D rendering
   - Real-time or accelerated time
   - Debug view showing agent thoughts
   ```

**Validation Tests:**
- Single agent can survive 100 sim-days
- Agent finds and consumes resources
- Agent avoids hazards
- Basic reasoning visible in debug logs

**Hardware:** Development on MacBook M4, testing on both machines

**Deliverable:** Working simulation with 1 functional agent

### Phase 3: Communication & Learning (Weeks 9-12)

**Goal:** Add second agent and learning systems

**Tasks:**
1. Implement Communication module
   ```python
   - Signal emission/reception
   - Meaning learning algorithm
   - Pattern recognition for signals
   ```

2. Build Experience tracking
   ```python
   - Experience recording
   - Significance evaluation
   - Memory management (core + buffer)
   ```

3. Implement Incremental retraining
   ```python
   - Experience ‚Üí training data formatting
   - Retraining triggers
   - Model update pipeline
   ```

**Validation Tests:**
- Two agents can perceive each other
- Signals are emitted and received
- Experiences are logged correctly
- Retraining completes in < 10 minutes

**Hardware:** Brother's PC for retraining, M4 for simulation

**Deliverable:** Two-agent system with learning capability

### Phase 4: Short Experiments (Weeks 13-16)

**Goal:** Run 100-day simulations and iterate

**Tasks:**
1. Run controlled experiments
   - Vary resource scarcity
   - Vary starting positions
   - Vary environmental pressures
   
2. Collect metrics
   - Survival rate
   - Interaction frequency
   - Signal usage patterns
   - Cooperation events
   
3. Analyze and iterate
   - Fix bugs
   - Tune parameters
   - Improve significance filtering
   - Optimize retraining

**Success Criteria:**
- Agents survive > 50 days
- At least 3 significant discoveries per agent
- Observable interaction patterns
- Retraining happens 3-5 times per 100 days

**Deliverable:** Stable system ready for long-term experiments

### Phase 5: Long-Term Evolution (Weeks 17-24+)

**Goal:** Run 500-1000+ day simulations

**Tasks:**
1. Extended simulation runs
   - Multiple 500+ day experiments
   - Vary initial conditions
   - Compare outcomes
   
2. Deep analysis
   - Document emergent behaviors
   - Identify social patterns
   - Map language development
   - Track belief evolution
   
3. Iteration and scaling
   - Add complexity if successful
   - Consider 3-4 agents
   - Add reproduction mechanism (optional)

**Research Questions:**
- Does cooperation emerge reliably?
- What communication patterns develop?
- Are there human-like social structures?
- Or completely alien solutions?

**Deliverable:** Research findings, documented behaviors, recorded simulations

---

## üìà Success Metrics

### Level 1: Basic Functionality (Minimum Viable)
- ‚úÖ Agents survive 100+ days
- ‚úÖ Retraining happens successfully
- ‚úÖ Knowledge accumulates without errors
- ‚úÖ System runs without crashes

### Level 2: Learning Observed (Good Progress)
- ‚úÖ Agents discover fire/tools/shelter
- ‚úÖ Repeated patterns show learning
- ‚úÖ Agents avoid previously harmful actions
- ‚úÖ Consistent behaviors emerge

### Level 3: Communication Develops (Significant Achievement)
- ‚úÖ Signals gain consistent meanings
- ‚úÖ Agents respond appropriately to signals
- ‚úÖ Communication increases over time
- ‚úÖ Proto-language observable

### Level 4: Cooperation Emerges (Major Success)
- ‚úÖ Resource sharing occurs
- ‚úÖ Reciprocity patterns develop
- ‚úÖ Agents coordinate actions
- ‚úÖ Mutual benefit demonstrated

### Level 5: Social Structures (Groundbreaking)
- ‚úÖ Trade patterns established
- ‚úÖ Property concepts emerge
- ‚úÖ Fairness norms develop
- ‚úÖ Conflict resolution mechanisms
- ‚úÖ Something genuinely novel observed

---

## üö® Potential Failure Modes & Solutions

### Failure 1: Agents Never Interact
**Symptom:** Agents ignore each other, survive independently

**Causes:**
- Environment too easy (abundant resources)
- No pressure for cooperation
- Agents don't need each other

**Solutions:**
- Increase resource scarcity
- Separate food/water sources
- Add environmental threats requiring cooperation
- Reduce individual carrying capacity

### Failure 2: Retraining Breaks Learning
**Symptom:** After retraining, agents behave worse or forget things

**Causes:**
- Catastrophic forgetting
- Poor experience formatting
- Training hyperparameters too aggressive

**Solutions:**
- Always include core knowledge in retraining
- Use very small learning rates (1e-5)
- Fewer epochs per retrain (2-3 max)
- Validate model after each retrain

### Failure 3: Agents Learn Wrong Patterns
**Symptom:** Agents develop incorrect beliefs, don't correct them

**Causes:**
- Coincidental associations
- Single-experience learning
- No belief-updating mechanism

**Solutions:**
- Require 3+ occurrences before pattern confirmation
- Implement belief contradiction detection
- Weight recent experiences more heavily
- Allow overwriting of old beliefs

### Failure 4: Communication Doesn't Emerge
**Symptom:** Signals remain random, no consistent meanings

**Causes:**
- No benefit to communication
- No feedback loop for signal learning
- Agents succeed without communicating

**Solutions:**
- Create scenarios where communication helps
- Reward successful signal interpretations
- Make some resources only accessible via cooperation
- Add shared threats requiring coordination

### Failure 5: Model Too Small/Dumb
**Symptom:** Agents can't reason about complex scenarios

**Causes:**
- Base model insufficient (< 500M parameters)
- Training data too limited
- Context window too small

**Solutions:**
- Use larger base model (1B-3B parameters)
- Enrich training data with more examples
- Implement better memory retrieval
- Consider hybrid reasoning system

### Failure 6: Exponential Data Growth
**Symptom:** Retraining takes hours, system slows down

**Causes:**
- Significance filter too permissive
- Not pruning old memories
- Including too much in retraining

**Solutions:**
- Increase significance threshold
- Implement aggressive pruning (keep top 200)
- Only retrain on core + recent (not everything)
- Optimize training code

---

## üíª Technical Stack

### Hardware Requirements

**Minimum (Development & Testing):**
- MacBook Pro M4 (for simulation and development)
- 16GB+ RAM
- 512GB+ storage

**Recommended (Training & Long Runs):**
- Desktop PC with RTX 3090/4090 or equivalent
- 32GB+ RAM
- 1TB+ storage
- Can SSH remotely for training jobs

### Software Stack

**Core Framework:**
- Python 3.10+
- PyTorch 2.0+ (for model training/inference)
- Transformers library (Hugging Face)

**For MacBook M4:**
- MLX (Apple's ML framework - optimized for Apple Silicon)
- OR PyTorch with MPS backend

**Simulation:**
- NumPy (world state management)
- Pygame (visualization)
- Custom simulation engine

**Training:**
- Accelerate (distributed training)
- bitsandbytes (quantization)
- wandb (experiment tracking - optional)

**Development:**
- Git (version control)
- Jupyter (experimentation)
- pytest (testing)

### Model Specifications

**Base Model:**
```
Architecture: Transformer-based (Llama-style)
Size: 500M - 1B parameters
Context Length: 2048-4096 tokens
Vocabulary: 32k tokens
Training: Continual pre-training on filtered dataset
```

**Inference:**
```
Quantization: 4-bit or 8-bit (for speed)
Batch Size: 1 (sequential agent decisions)
Context: Recent experiences + core knowledge
Generation: ~50-200 tokens per decision
```

---

## üìä Logging & Analysis

### What to Log

**Every Simulation Step:**
```python
{
    'sim_day': 247,
    'timestamp': '2024-11-27T10:34:22',
    'agent_id': 'agent_A',
    'position': (23, 15),
    'state': {
        'hunger': 45,
        'thirst': 30,
        'health': 85
    },
    'observation': [...],
    'thought_process': "I observe food nearby and another agent...",
    'action': 'emit_signal_üî¥',
    'outcome': 'agent_B_approached',
    'reward': +15
}
```

**After Each Retraining:**
```python
{
    'retrain_id': 7,
    'sim_day': 248,
    'trigger': 'cooperation_pattern_confirmed',
    'experiences_trained': 73,
    'training_time_seconds': 420,
    'model_version': 'v1.7',
    'new_knowledge': [
        "Signal üî¥ reliably indicates food sharing opportunity",
        "Cooperation with agent_B increases survival by ~20%"
    ]
}
```

**Periodic Summaries (Daily):**
```python
{
    'sim_day': 250,
    'summary': {
        'agent_A': {
            'survival_rate': 0.95,
            'interactions': 18,
            'signals_emitted': 42,
            'discoveries': 3,
            'cooperation_events': 7
        },
        'agent_B': {
            'survival_rate': 0.92,
            'interactions': 18,
            'signals_emitted': 38,
            'discoveries': 2,
            'cooperation_events': 7
        },
        'emergent_patterns': [
            "Reciprocal resource sharing established",
            "Signal meanings becoming consistent"
        ]
    }
}
```

### Analysis Tools

**Visualization:**
- Replay simulation with agent thoughts visible
- Plot survival rates over time
- Graph interaction frequencies
- Signal usage heatmaps
- Knowledge accumulation curves

**Metrics:**
- Cooperation coefficient (% of interactions that are cooperative)
- Communication consistency (signal ‚Üí response reliability)
- Learning rate (new discoveries per 100 days)
- Social complexity (unique interaction patterns)

---

## üéì Expected Learnings

### Even If "It Doesn't Work"

**You'll Learn:**
1. How to train small language models from scratch
2. Multi-agent simulation design
3. Reinforcement learning principles
4. Continual learning systems
5. Emergent behavior analysis

### If Basic Cooperation Emerges

**You'll Discover:**
- What minimal conditions enable cooperation
- How communication develops from nothing
- Whether reciprocity arises naturally
- How trust builds between agents

### If Complex Behaviors Emerge

**You'll Answer:**
- Are human social structures inevitable or contingent?
- What's universal to intelligent cooperation?
- What patterns are uniquely human vs. universal?
- Can different "civilizations" solve problems differently?

---

## üåü Future Extensions

### After V1 Success:

**More Agents (3-5):**
- Does cooperation scale?
- Do coalitions form?
- Does hierarchy emerge?

**Reproduction:**
- Pass learned knowledge to offspring
- Cultural transmission
- Evolution of strategies

**Complex Environment:**
- Seasons, weather, disasters
- Multiple biomes
- Predators or threats

**Tool Creation:**
- Agents invent tools
- Technology tree discovery
- Innovation diffusion

**Longer Timescales:**
- Multi-generational (if reproduction added)
- 10,000+ simulation days
- Cultural evolution

---

## ü§î Open Questions

**Scientific:**
- Is cooperation inevitable given intelligence + scarcity?
- What's the minimal cognitive architecture for social behavior?
- Are human institutions (property, trade, law) universal or cultural?

**Technical:**
- Can 1B models develop genuine creativity?
- How much knowledge can be learned vs. must be pre-programmed?
- What's the optimal retraining frequency?

**Philosophical:**
- What does this tell us about consciousness?
- Is learned knowledge "real" understanding?
- Are we in a simulation? (just kidding... or are we?)

---

## üìö References & Inspirations

**Academic Research:**
- "Generative Agents: Interactive Simulacra of Human Behavior" (Stanford, 2023)
- Game theory and emergence of cooperation (Axelrod)
- Multi-agent reinforcement learning literature
- Continual learning in neural networks

**Differences from Existing Work:**
- Stanford's Smallville uses GPT-4 (pre-loaded with human knowledge)
- Most RL agents can't reason abstractly
- No existing work combines: blank slate + reasoning + continuous learning

**This project's novelty:**
- True minimal knowledge starting point
- Experience-based knowledge building
- Tests genuine emergence vs. pattern matching

---

## üöÄ Getting Started

### Quick Start (Week 1)

1. **Set up development environment**
   ```bash
   git clone [your-repo]
   cd project-genesis
   pip install -r requirements.txt
   ```

2. **Run simple test simulation**
   ```bash
   python test_world.py
   # Should see: 50x50 grid, 2 agents, basic survival
   ```

3. **Download/prepare base model**
   ```bash
   python prepare_model.py --size 1B --filter-dataset
   # Will take 1-2 weeks to fully train
   ```

4. **Run first experiment**
   ```bash
   python run_simulation.py --days 100 --agents 2
   # Observe behaviors, check logs
   ```

### Join the Project

**This is ambitious. You don't have to do it alone.**

Consider:
- Open sourcing on GitHub
- Recruiting collaborators
- Seeking academic advisors
- Applying for research grants
- Documenting as you build (blog/videos)

---

## ‚úÖ Summary: The Path Forward

**What You're Building:**
Two AI agents with minimal starting knowledge, placed in a simulated Earth, who learn ONLY from experience through continuous model retraining.

**Why It's Hard:**
Combines blank-slate learning, complex reasoning, and emergent social behavior - nobody has done this exact approach.

**Why It's Feasible:**
Small models (1B), selective retraining (only significant experiences), incremental learning (mimics humans), your available hardware.

**Why It Matters:**
Tests whether human civilization is inevitable or contingent. Could reveal universal principles of cooperation, or show us we could have done things completely differently.

**Timeline:**
6-9 months to meaningful results, 12+ months for deep exploration.

**The Real Goal:**
Not to perfectly simulate human civilization, but to understand what emerges when intelligence meets scarcity without cultural templates.

---

## üí¨ Final Thoughts

This project asks one of the most profound questions in AI and philosophy: **What is inevitable about civilization, and what is accident?**

Whether you discover that agents recreate human-like structures (suggesting inevitability) or build something completely alien (suggesting contingency), both answers are fascinating.

The journey of building this - even if it "fails" - will teach you more about AI, emergence, and intelligence than most formal education.

**Start small. Learn fast. Iterate often. Document everything.**

And remember: the most interesting results are often the unexpected ones.

---

**Project Status:** Concept & Design Phase  
**Next Milestone:** Base model training  
**Timeline:** 6-9 months to first results  
**Last Updated:** November 2024

---

*"The question is not whether we can build artificial intelligence that thinks like us, but whether intelligence itself inevitably leads to certain forms of cooperation, communication, and civilization."*
